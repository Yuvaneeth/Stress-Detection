{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading max 1000 tweets\n",
      "Downloaded 87 tweets\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-60f4607a5a8d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;31m# calling main function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-32-60f4607a5a8d>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    326\u001b[0m                 \u001b[0mtweetCount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Downloaded {0} tweets\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweetCount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                 \u001b[0mmax_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_tweets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m86\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTweepError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 \u001b[1;31m# Just exit if any error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'id'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import jsonpickle\n",
    "import os\n",
    "import re \n",
    "import tweepy \n",
    "from tweepy import OAuthHandler \n",
    "from textblob import TextBlob \n",
    "import json\n",
    "import datetime\n",
    "import pandas\n",
    "import emoji\n",
    "\n",
    "#defining attributes\n",
    "#positive and negative emotion words\n",
    "emotion_words = [0,0]\n",
    "#positive and negative emoticons and emojis\n",
    "emoticons = [0,0]\n",
    "#punctuation marks !,?,...,.\n",
    "punc_marks=[0,0,0,0]\n",
    "#degree adverbs\n",
    "degree=[0,0]\n",
    "#no of comments,retweets and likes\n",
    "social_attn=[0,0,0]\n",
    "\n",
    "\n",
    "\n",
    "#HappyEmoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "POSITIVE = [\"*O\", \"*-*\", \"*O*\", \"*o*\", \"* *\",\n",
    "                \":P\", \":D\", \":d\", \":p\",\n",
    "                \";P\", \";D\", \";d\", \";p\",\n",
    "                \":-)\", \";-)\", \":=)\", \";=)\",\n",
    "                \":<)\", \":>)\", \";>)\", \";=)\",\n",
    "                \"=}\", \":)\", \"(:;)\",\n",
    "                \"(;\", \":}\", \"{:\", \";}\",\n",
    "                \"{;:]\",\n",
    "                \"[;\", \":')\", \";')\", \":-3\",\n",
    "                \"{;\", \":]\",\n",
    "                \";-3\", \":-x\", \";-x\", \":-X\",\n",
    "                \";-X\", \":-}\", \";-=}\", \":-]\",\n",
    "                \";-]\", \":-.)\",\n",
    "                \"^_^\", \"^-^\"]\n",
    "\n",
    "NEGATIVE = [\":(\", \";(\", \":'(\",\n",
    "                \"=(\", \"={\", \"):\", \");\",\n",
    "                \")':\", \")';\", \")=\", \"}=\",\n",
    "                \";-{{\", \";-{\", \":-{{\", \":-{\",\n",
    "                \":-(\", \";-(\",\n",
    "                \":,)\", \":'{\",\n",
    "                \"[:\", \";]\"\n",
    "]\n",
    "\n",
    "emopos = [\"😀\",\"😁\",\"😂\",\"🤣\",\"😃\",\"😄\",\"😅\",\"😆\",\"😉\",\"😊\",\"😋\",\"😎\",\"😍\",\"😘\",\"🥰\",\"😗\",\"😙\",\"😚\",\"☺️\",\"🙂\",\"🤗\",\"🤩\",\"😴\"\n",
    "          \"😌\",\"😛\",\"😜\",\"😝\",\"🤤\",\"🤑\",\"😬\",\"😇\",\"🤠\",\"🤡\",\"🥳\",\"🤭\",\"🧐\",\"🤓\",\"😈\"\"👻\",\"👽\",\"🤖\",\"😺\",\"😸\",\"😹\",\"😻\",\"😽\"]\n",
    "\n",
    "\n",
    "\n",
    "emoneg = [\"🤔\",\"🤨\",\"😐\",\"😑\",\"😶\",\"🙄\",\"😏\",\"😣\",\"😥\",\"😮\",\"🤐\",\"😯\",\"😪\",\"😫\",\"😒\",\"😓\",\"😔\",\"😕\",\"🙃\",\"😲\",\"☹️\",\"🙁\",\"😖\",\"😞\",\"😟\",\"😤\",\"😢\",\"😭\",\"😦\",\"😧\",\"😨\",\"😩\",\"🤯\",\n",
    "          \"😰\",\"😱\",\"🥵\",\"🥶\",\"😳\",\"🤪\",\"😵\",\"😡\",\"😠\",\"🤬\",\"😷\",\"🤒\",\"🤕\",\"🤢\",\"🤮\",\"🤧\",\"🥴\",\"🥺\",\"🤥\",\"🤫\",\"👿\",\"👹\",\"👺\",\"💀\",\"😼\",\"🙀\",\"😿\",\"😾\"]\n",
    "\n",
    "\n",
    "  \n",
    "class TwitterClient(object): \n",
    "    ''' \n",
    "    Generic Twitter Class for sentiment analysis. \n",
    "    '''      \n",
    "        \n",
    "    \n",
    "        \n",
    "    def __init__(self): \n",
    "        ''' \n",
    "        Class constructor or initialization method. \n",
    "        '''\n",
    "        # keys and tokens from the Twitter Dev Console \n",
    "        consumer_key = 'koB9DtzCdzBpYWWWuGZaAZfQ4'\n",
    "        consumer_secret = 'sidV3z48EbrcBtnp7ga9843CVe0ddoVungGIeMMwSbvS3Fu1En'\n",
    "        access_token = '922408940616351744-XOeNML7LFT92plNNzklsN6BTLt8QjD2'\n",
    "        access_token_secret = 'RpddoO7TsSory7sFLbNYNGL9gZxlsdQ1LeCDtZ7uAoV05'\n",
    "  \n",
    "        # attempt authentication \n",
    "        try: \n",
    "            # create OAuthHandler object \n",
    "            self.auth = OAuthHandler(consumer_key, consumer_secret) \n",
    "            # set access token and secret \n",
    "            self.auth.set_access_token(access_token, access_token_secret) \n",
    "            # create tweepy API object to fetch tweets \n",
    "            self.api = tweepy.API(self.auth) \n",
    "        except: \n",
    "            print(\"Error: Authentication Failed\") \n",
    "  \n",
    "    def clean_tweet(self, tweet): \n",
    "        ''' \n",
    "        Utility function to clean tweet text by removing links, special characters \n",
    "        using simple regex statements. \n",
    "        '''\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split()) \n",
    "         \n",
    "  \n",
    "    def get_tweet_sentiment(self, tweet): \n",
    "        ''' \n",
    "        Utility function to classify sentiment of passed tweet \n",
    "        using textblob's sentiment method \n",
    "        '''\n",
    "        # create TextBlob object of passed tweet text \n",
    "        analysis = TextBlob(self.clean_tweet(tweet)) \n",
    "        # set sentiment \n",
    "        if analysis.sentiment.polarity > 0: \n",
    "            return 'positive'\n",
    "        elif analysis.sentiment.polarity == 0: \n",
    "            return 'neutral'\n",
    "        else: \n",
    "            return 'negative'\n",
    "                                               \n",
    "\n",
    "    def get_punc(str,pun):\n",
    "        count = 0\n",
    "        if pun =='...':\n",
    "            for j in range(0,len(str)-2):\n",
    "                if(str[j]==\".\" and str[j+1]==\".\" and str[j+2]==\".\"):\n",
    "                    count = count + 1\n",
    "        else:          \n",
    "            for i in range (0, len (str)):   \n",
    "                #Checks whether given character is a punctuation mark  \n",
    "                if str[i] in (pun):  \n",
    "                    count = count + 1\n",
    "                \n",
    "        return count\n",
    "    \n",
    "    def extract_emojis(str):\n",
    "        return ''.join(c for c in str if c in emoji.UNICODE_EMOJI)\n",
    "    \n",
    "    def get_pemoji(str):\n",
    "        count = 0\n",
    "        co = TextBlob(str)\n",
    "        for i in POSITIVE:\n",
    "            if i in co:\n",
    "                count = count +1\n",
    "        for j in emoticons_happy:\n",
    "            if i in co:\n",
    "                count = count +1\n",
    "        n = TwitterClient.extract_emojis(str)\n",
    "        for k in emopos:\n",
    "            if k in n:\n",
    "                count = count +1\n",
    "        return count\n",
    "    def get_nemoji(str):\n",
    "        count = 0\n",
    "        co = TextBlob(str)\n",
    "        for i in NEGATIVE:\n",
    "            if i in co:\n",
    "                count = count +1\n",
    "        for j in emoticons_sad:\n",
    "            if i in co:\n",
    "                count = count +1\n",
    "        n = TwitterClient.extract_emojis(str)\n",
    "        for k in emoneg:\n",
    "            if k in n:\n",
    "                count = count +1\n",
    "        return count\n",
    "    \n",
    "    def get_pwords(str):\n",
    "        count = 0\n",
    "        a = TextBlob(str)\n",
    "        for word in a.words:\n",
    "            b = TextBlob(word)            \n",
    "            if(b.sentiment.polarity>0):\n",
    "                count = count + 1\n",
    "        return count\n",
    "    def get_nwords(str):\n",
    "        count = 0\n",
    "        a = TextBlob(str)\n",
    "        for word in a.words:\n",
    "            b = TextBlob(word)            \n",
    "            if(b.sentiment.polarity>0):\n",
    "                count = count + 1\n",
    "        return count\n",
    "    def gettime(date):\n",
    "        return date._str_()\n",
    "        \n",
    " \n",
    " \n",
    "    \n",
    "  \n",
    "    def get_tweets(self, query, count = 30,mid=None,sid=None): \n",
    "        ''' \n",
    "        Main function to fetch tweets and parse them. \n",
    "        '''\n",
    "        # empty list to store parsed tweets \n",
    "        tweets = [] \n",
    "  \n",
    "        try: \n",
    "            # call twitter api to fetch tweets \n",
    "            fetched_tweets = self.api.search(q = query, count = count,max_id=mid,since_id=sid) \n",
    "  \n",
    "            # parsing tweets one by one \n",
    "            for tweet in fetched_tweets: \n",
    "                # empty dictionary to store required params of a tweet \n",
    "                parsed_tweet = {} \n",
    "                # saving text of tweet \n",
    "                parsed_tweet['text'] = tweet.text \n",
    "                # saving sentiment of tweet \n",
    "                parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text) \n",
    "                # savint tweet time\n",
    "                #parsed_tweet['time'] =TwitterClient.gettime(tweet.created_at)\n",
    "                #saving tweet's user id\n",
    "                parsed_tweet['userid'] = tweet.id_str\n",
    "                #saving emojis\n",
    "                #parsed_tweet['emoji']=tweet.entities['symbols']\n",
    "                #saving retweet count\n",
    "                parsed_tweet['retweets_count'] = tweet.retweet_count\n",
    "                #saving favorite count\n",
    "                parsed_tweet['favorite_count'] = tweet.favorite_count\n",
    "                #saving reply count\n",
    "                #parsed_tweet['reply_count'] = tweet.reply_count\n",
    "                #counting number of punctuation marks\n",
    "                parsed_tweet['punc1']=TwitterClient.get_punc(tweet.text,'!')\n",
    "                parsed_tweet['punc2']=TwitterClient.get_punc(tweet.text,'?')\n",
    "                parsed_tweet['punc3']=TwitterClient.get_punc(tweet.text,'...')\n",
    "                parsed_tweet['punc4']=TwitterClient.get_punc(tweet.text,'.') - 3*TwitterClient.get_punc(tweet.text,'...')\n",
    "                #counting positve emojis\n",
    "                parsed_tweet['pemoji']= TwitterClient.get_pemoji(tweet.text)\n",
    "                #counting negative emojis\n",
    "                parsed_tweet['nemoji'] = TwitterClient.get_nemoji(tweet.text)\n",
    "                #counting positive words\n",
    "                parsed_tweet['pword'] = TwitterClient.get_pwords(tweet.text)\n",
    "                #counting negative words\n",
    "                parsed_tweet['nword'] = TwitterClient.get_nwords(tweet.text)\n",
    "                parsed_tweet['id']=tweet.id\n",
    "                #counting  positive degree adverbs\n",
    "                #parsed_tweet['padv'] = TwitterClient.get_padv(tweet.text)\n",
    "                #counting negative degree adverbs\n",
    "                #parsed_tweet['nadv'] = TwitterClient.get_nadv(tweet.text)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                #def function to convert json-serialilze datetime\n",
    "                def default(o):\n",
    "                    if isinstance(o, datetime.datetime):\n",
    "                        return o.__str__()\n",
    "   \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                with open('twe6.json','a') as t:\n",
    "  \n",
    "                # appending parsed tweet to tweets list \n",
    "                    if tweet.retweet_count > 0: \n",
    "                        # if tweet has retweets, ensure that it is appended only once \n",
    "                        if parsed_tweet not in tweets: \n",
    "                            tweets.append(parsed_tweet)\n",
    "                            qwe=json.dump(parsed_tweet,t)                            \n",
    "                            t.write('\\n')\n",
    "                    else: \n",
    "                        tweets.append(parsed_tweet) \n",
    "                        qwe=json.dump(parsed_tweet,t)                        \n",
    "                        t.write('\\n')\n",
    "               \n",
    "   \n",
    "            # return parsed tweets \n",
    "            return tweets\n",
    "  \n",
    "        except tweepy.TweepError as e: \n",
    "            # print error (if any) \n",
    "            print(\"Error : \" + str(e)) \n",
    "  \n",
    "def main(): \n",
    "    # creating object of TwitterClient Class \n",
    "    api = TwitterClient() \n",
    "    # calling function to get tweets \n",
    "    #tweets = api.get_tweets(query = 'I feel stressed',count=100) \n",
    "    \n",
    "    \n",
    "\n",
    "    searchQuery = 'i feel stress'  # this is what we're searching for\n",
    "    maxTweets = 1000 # Some arbitrary large number\n",
    "    tweetsPerQry = 100  # this is the max the API permits\n",
    "    fName = 'tweet.txt' # We'll store the tweets in a text file.\n",
    "\n",
    "\n",
    "    # If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "    # else default to no lower limit, go as far back as API allows\n",
    "    sinceId = None\n",
    "\n",
    "    # If results only below a specific ID are, set max_id to that ID.\n",
    "    # else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "    max_id = -1\n",
    "\n",
    "    tweetCount = 0\n",
    "    print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "    with open(fName, 'w') as f:\n",
    "        while tweetCount < maxTweets:\n",
    "            try:\n",
    "                if (max_id <= 0):\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = api.get_tweets(searchQuery, count=tweetsPerQry)\n",
    "                    else:\n",
    "                        new_tweets = api.get_tweets(searchQuery, count=tweetsPerQry,\n",
    "                                                since_id=sinceId)\n",
    "                else:\n",
    "                    if (not sinceId):\n",
    "                        new_tweets = api.get_tweets(searchQuery, count=tweetsPerQry, max_id=str(max_id - 1))\n",
    "                    else:\n",
    "                        new_tweets = api.get_tweets(searchQuery, count=tweetsPerQry,max_id=str(max_id - 1),since_id=sinceId)\n",
    "                if not new_tweets:\n",
    "                    print(\"No more tweets found\")\n",
    "                    break\n",
    "                for tweet in new_tweets:\n",
    "                    json.dump(tweet,f)\n",
    "                    #f.write(jsonpickle.encode(tweet._json, unpicklable=False) +'\\n')\n",
    "                tweetCount += len(new_tweets)\n",
    "                print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "                max_id = new_tweets[-86].id\n",
    "            except tweepy.TweepError as e:\n",
    "                # Just exit if any error\n",
    "                print(\"some error : \" + str(e))\n",
    "                break\n",
    "\n",
    "  \n",
    "  \n",
    "if __name__ == \"__main__\": \n",
    "    # calling main function \n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading max 1000 tweets\n",
      "Downloaded 72 tweets\n",
      "Downloaded 144 tweets\n",
      "Downloaded 219 tweets\n",
      "Downloaded 296 tweets\n",
      "Downloaded 360 tweets\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-145-5846fff23227>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;31m# calling main function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-145-5846fff23227>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    339\u001b[0m     \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwitterClient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[1;31m# calling function to get tweets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m     \u001b[0mtweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'I feel stressed'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m     \u001b[1;31m#print(tweets[1]['text'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-145-5846fff23227>\u001b[0m in \u001b[0;36mget_tweets\u001b[1;34m(self, query, count)\u001b[0m\n\u001b[0;32m    285\u001b[0m                         \u001b[0mparsed_tweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nemoji'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwitterClient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_nemoji\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m                         \u001b[1;31m#counting positive words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m                         \u001b[0mparsed_tweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pword'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwitterClient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_pwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m                         \u001b[1;31m#counting negative words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m                         \u001b[0mparsed_tweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'nword'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwitterClient\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_nwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-145-5846fff23227>\u001b[0m in \u001b[0;36mget_pwords\u001b[1;34m(str)\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolarity\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m                 \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yuvaneeth\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, text, tokenizer, pos_tagger, np_extractor, analyzer, parser, classifier, clean_html)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstripped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlowerstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         _initialize_models(self, tokenizer, pos_tagger, np_extractor, analyzer,\n\u001b[1;32m--> 378\u001b[1;33m                            parser, classifier)\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mcached_property\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yuvaneeth\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\textblob\\blob.py\u001b[0m in \u001b[0;36m_initialize_models\u001b[1;34m(obj, tokenizer, pos_tagger, np_extractor, analyzer, parser, classifier)\u001b[0m\n\u001b[0;32m    322\u001b[0m     obj.tokenizer = _validated_param(tokenizer, \"tokenizer\",\n\u001b[0;32m    323\u001b[0m                                     \u001b[0mbase_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTokenizerI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                                     \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBaseBlob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m                                     base_class_name=\"BaseTokenizer\")\n\u001b[0;32m    326\u001b[0m     obj.np_extractor = _validated_param(np_extractor, \"np_extractor\",\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re \n",
    "import tweepy \n",
    "from tweepy import OAuthHandler \n",
    "from textblob import TextBlob \n",
    "import json\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import pandas\n",
    "import emoji\n",
    "\n",
    "#defining attributes\n",
    "#positive and negative emotion words\n",
    "emotion_words = [0,0]\n",
    "#positive and negative emoticons and emojis\n",
    "emoticons = [0,0]\n",
    "#punctuation marks !,?,...,.\n",
    "punc_marks=[0,0,0,0]\n",
    "#degree adverbs\n",
    "degree=[0,0]\n",
    "#no of comments,retweets and likes\n",
    "social_attn=[0,0,0]\n",
    "\n",
    "\n",
    "\n",
    "#HappyEmoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "POSITIVE = [\"*O\", \"*-*\", \"*O*\", \"*o*\", \"* *\",\n",
    "                \":P\", \":D\", \":d\", \":p\",\n",
    "                \";P\", \";D\", \";d\", \";p\",\n",
    "                \":-)\", \";-)\", \":=)\", \";=)\",\n",
    "                \":<)\", \":>)\", \";>)\", \";=)\",\n",
    "                \"=}\", \":)\", \"(:;)\",\n",
    "                \"(;\", \":}\", \"{:\", \";}\",\n",
    "                \"{;:]\",\n",
    "                \"[;\", \":')\", \";')\", \":-3\",\n",
    "                \"{;\", \":]\",\n",
    "                \";-3\", \":-x\", \";-x\", \":-X\",\n",
    "                \";-X\", \":-}\", \";-=}\", \":-]\",\n",
    "                \";-]\", \":-.)\",\n",
    "                \"^_^\", \"^-^\"]\n",
    "\n",
    "NEGATIVE = [\":(\", \";(\", \":'(\",\n",
    "                \"=(\", \"={\", \"):\", \");\",\n",
    "                \")':\", \")';\", \")=\", \"}=\",\n",
    "                \";-{{\", \";-{\", \":-{{\", \":-{\",\n",
    "                \":-(\", \";-(\",\n",
    "                \":,)\", \":'{\",\n",
    "                \"[:\", \";]\"\n",
    "]\n",
    "\n",
    "emopos = [\"😀\",\"😁\",\"😂\",\"🤣\",\"😃\",\"😄\",\"😅\",\"😆\",\"😉\",\"😊\",\"😋\",\"😎\",\"😍\",\"😘\",\"🥰\",\"😗\",\"😙\",\"😚\",\"☺️\",\"🙂\",\"🤗\",\"🤩\",\"😴\"\n",
    "          \"😌\",\"😛\",\"😜\",\"😝\",\"🤤\",\"🤑\",\"😬\",\"😇\",\"🤠\",\"🤡\",\"🥳\",\"🤭\",\"🧐\",\"🤓\",\"😈\"\"👻\",\"👽\",\"🤖\",\"😺\",\"😸\",\"😹\",\"😻\",\"😽\"]\n",
    "\n",
    "\n",
    "\n",
    "emoneg = [\"🤔\",\"🤨\",\"😐\",\"😑\",\"😶\",\"🙄\",\"😏\",\"😣\",\"😥\",\"😮\",\"🤐\",\"😯\",\"😪\",\"😫\",\"😒\",\"😓\",\"😔\",\"😕\",\"🙃\",\"😲\",\"☹️\",\"🙁\",\"😖\",\"😞\",\"😟\",\"😤\",\"😢\",\"😭\",\"😦\",\"😧\",\"😨\",\"😩\",\"🤯\",\n",
    "          \"😰\",\"😱\",\"🥵\",\"🥶\",\"😳\",\"🤪\",\"😵\",\"😡\",\"😠\",\"🤬\",\"😷\",\"🤒\",\"🤕\",\"🤢\",\"🤮\",\"🤧\",\"🥴\",\"🥺\",\"🤥\",\"🤫\",\"👿\",\"👹\",\"👺\",\"💀\",\"😼\",\"🙀\",\"😿\",\"😾\"]\n",
    "\n",
    "\n",
    "  \n",
    "class TwitterClient(object): \n",
    "    ''' \n",
    "    Generic Twitter Class for sentiment analysis. \n",
    "    '''      \n",
    "        \n",
    "    \n",
    "        \n",
    "    def __init__(self): \n",
    "        ''' \n",
    "        Class constructor or initialization method. \n",
    "        '''\n",
    "        # keys and tokens from the Twitter Dev Console \n",
    "        consumer_key = 'koB9DtzCdzBpYWWWuGZaAZfQ4'\n",
    "        consumer_secret = 'sidV3z48EbrcBtnp7ga9843CVe0ddoVungGIeMMwSbvS3Fu1En'\n",
    "        access_token = '922408940616351744-XOeNML7LFT92plNNzklsN6BTLt8QjD2'\n",
    "        access_token_secret = 'RpddoO7TsSory7sFLbNYNGL9gZxlsdQ1LeCDtZ7uAoV05'\n",
    "  \n",
    "        # attempt authentication \n",
    "        try: \n",
    "            # create OAuthHandler object \n",
    "            self.auth = OAuthHandler(consumer_key, consumer_secret) \n",
    "            # set access token and secret \n",
    "            self.auth.set_access_token(access_token, access_token_secret) \n",
    "            # create tweepy API object to fetch tweets \n",
    "            self.api = tweepy.API(self.auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True) \n",
    "        except: \n",
    "            print(\"Error: Authentication Failed\") \n",
    "  \n",
    "    def clean_tweet(self, tweet): \n",
    "        ''' \n",
    "        Utility function to clean tweet text by removing links, special characters \n",
    "        using simple regex statements. \n",
    "        '''\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split()) \n",
    "         \n",
    "  \n",
    "    def get_tweet_sentiment(self, tweet): \n",
    "        ''' \n",
    "        Utility function to classify sentiment of passed tweet \n",
    "        using textblob's sentiment method \n",
    "        '''\n",
    "        # create TextBlob object of passed tweet text \n",
    "        analysis = TextBlob(self.clean_tweet(tweet)) \n",
    "        # set sentiment \n",
    "        if analysis.sentiment.polarity > 0: \n",
    "            return 'positive'\n",
    "        elif analysis.sentiment.polarity == 0: \n",
    "            return 'neutral'\n",
    "        else: \n",
    "            return 'negative'\n",
    "                                               \n",
    "\n",
    "    def get_punc(str,pun):\n",
    "        count = 0\n",
    "        if pun =='...':\n",
    "            for j in range(0,len(str)-2):\n",
    "                if(str[j]==\".\" and str[j+1]==\".\" and str[j+2]==\".\"):\n",
    "                    count = count + 1\n",
    "        else:          \n",
    "            for i in range (0, len (str)):   \n",
    "                #Checks whether given character is a punctuation mark  \n",
    "                if str[i] in (pun):  \n",
    "                    count = count + 1\n",
    "                \n",
    "        return count\n",
    "    \n",
    "    def extract_emojis(str):\n",
    "        return ''.join(c for c in str if c in emoji.UNICODE_EMOJI)\n",
    "    \n",
    "    def get_pemoji(str):\n",
    "        count = 0\n",
    "        co = TextBlob(str)\n",
    "        for i in POSITIVE:\n",
    "            if i in co:\n",
    "                count = count +1\n",
    "        for j in emoticons_happy:\n",
    "            if i in co:\n",
    "                count = count +1\n",
    "        n = TwitterClient.extract_emojis(str)\n",
    "        for k in emopos:\n",
    "            if k in n:\n",
    "                count = count +1\n",
    "        return count\n",
    "    def get_nemoji(str):\n",
    "        count = 0\n",
    "        co = TextBlob(str)\n",
    "        for i in NEGATIVE:\n",
    "            if i in co:\n",
    "                count = count +1\n",
    "        for j in emoticons_sad:\n",
    "            if i in co:\n",
    "                count = count +1\n",
    "        n = TwitterClient.extract_emojis(str)\n",
    "        for k in emoneg:\n",
    "            if k in n:\n",
    "                count = count +1\n",
    "        return count\n",
    "    \n",
    "    def get_pwords(str):\n",
    "        count = 0\n",
    "        a = TextBlob(str)\n",
    "        for word in a.words:\n",
    "            b = TextBlob(word)            \n",
    "            if(b.sentiment.polarity>0):\n",
    "                count = count + 1\n",
    "        return count\n",
    "    def get_nwords(str):\n",
    "        count = 0\n",
    "        a = TextBlob(str)\n",
    "        for word in a.words:\n",
    "            b = TextBlob(word)            \n",
    "            if(b.sentiment.polarity>0):\n",
    "                count = count + 1\n",
    "        return count\n",
    "    def get_padv(str):\n",
    "        a = TextBlob(str)\n",
    "        if(a.sentiment.polarity>0):\n",
    "            for word in a.tags:\n",
    "                if word[1]=='JJS'or word[1]=='RBS':\n",
    "                    return 3\n",
    "                elif word[1]=='JJR'or word[1]=='RBR':\n",
    "                    return 2\n",
    "                else: \n",
    "                    return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "       \n",
    "            \n",
    "    def get_nadv(str):\n",
    "        a = TextBlob(str)\n",
    "        if(a.sentiment.polarity<0):        \n",
    "            for word in a.tags:\n",
    "                if word[1]=='JJS'or word[1]=='RBS':\n",
    "                    return -3\n",
    "                elif word[1]=='JJR'or word[1]=='RBR':\n",
    "                    return -2\n",
    "                else: \n",
    "                    return -1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "        \n",
    "    def get_tweets(self, query, count = 30): \n",
    "        ''' \n",
    "        Main function to fetch tweets and parse them. \n",
    "        '''\n",
    "        #empty list to store parsed tweets \n",
    "        tweets = []  \n",
    "        # call twitter api to fetch tweets \n",
    "        #fetched_tweets = self.api.search(q = query, count = count)         \n",
    "            \n",
    "        # Continue with rest of code\n",
    "        searchQuery = 'i feel relaxed'  # this is what we're searching for\n",
    "        maxTweets = 1000 # Some arbitrary large number\n",
    "        tweetsPerQry = 100  # this is the max the API permits\n",
    "        fName = 'ex.json' # We'll store the tweets in a text file.\n",
    "\n",
    "\n",
    "        # If results from a specific ID onwards are reqd, set since_id to that ID.\n",
    "        # else default to no lower limit, go as far back as API allows\n",
    "        sinceId = None\n",
    "            # If results only below a specific ID are, set max_id to that ID.\n",
    "            # else default to no upper limit, start from the most recent tweet matching the search query.\n",
    "        max_id = -1\n",
    "\n",
    "        tweetCount = 0\n",
    "        print(\"Downloading max {0} tweets\".format(maxTweets))\n",
    "        with open(fName, 'a') as f:\n",
    "            while tweetCount < maxTweets:\n",
    "                try:\n",
    "                    if (max_id <= 0):\n",
    "                        if (not sinceId):\n",
    "                            new_tweets = self.api.search(q=searchQuery, count=tweetsPerQry)\n",
    "                        else:\n",
    "                            new_tweets = self.api.search(q=searchQuery, count=tweetsPerQry,since_id=sinceId)\n",
    "                    else:\n",
    "                        if (not sinceId):\n",
    "                            new_tweets = self.api.search(q=searchQuery, count=tweetsPerQry,max_id=str(max_id - 1))\n",
    "                        else:\n",
    "                            new_tweets = self.api.search(q=searchQuery, count=tweetsPerQry,max_id=str(max_id - 1),since_id=sinceId)\n",
    "                    if not new_tweets:\n",
    "                        print(\"No more tweets found\")\n",
    "                        break\n",
    "                    for tweet in new_tweets:\n",
    "                        \n",
    "                        parsed_tweet = {} \n",
    "                        # saving text of tweet \n",
    "                        parsed_tweet['text'] = tweet.text \n",
    "                        # saving sentiment of tweet \n",
    "                        parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text)\n",
    "                        # savint tweet time\n",
    "                        parsed_tweet['time'] =tweet.created_at.strftime(\"%d-%b-%Y (%H:%M:%S.%f)\")\n",
    "                        #saving tweet's user id\n",
    "                        parsed_tweet['userid'] = tweet.id_str\n",
    "                        #saving emojis\n",
    "                        #parsed_tweet['emoji']=tweet.entities['symbols']\n",
    "                        #saving retweet count\n",
    "                        parsed_tweet['retweets_count'] = tweet.retweet_count\n",
    "                        #saving favorite count\n",
    "                        parsed_tweet['favorite_count'] = tweet.favorite_count\n",
    "                        #saving reply count\n",
    "                        #parsed_tweet['reply_count'] = tweet.reply_count\n",
    "                        #counting number of punctuation marks\n",
    "                        parsed_tweet['punc1']=TwitterClient.get_punc(tweet.text,'!')\n",
    "                        parsed_tweet['punc2']=TwitterClient.get_punc(tweet.text,'?')\n",
    "                        parsed_tweet['punc3']=TwitterClient.get_punc(tweet.text,'...')\n",
    "                        parsed_tweet['punc4']=TwitterClient.get_punc(tweet.text,'.') - 3*TwitterClient.get_punc(tweet.text,'...')\n",
    "                        #counting positve emojis\n",
    "                        parsed_tweet['pemoji']= TwitterClient.get_pemoji(tweet.text)\n",
    "                        #counting negative emojis\n",
    "                        parsed_tweet['nemoji'] = TwitterClient.get_nemoji(tweet.text)\n",
    "                        #counting positive words\n",
    "                        parsed_tweet['pword'] = TwitterClient.get_pwords(tweet.text)\n",
    "                        #counting negative words\n",
    "                        parsed_tweet['nword'] = TwitterClient.get_nwords(tweet.text)\n",
    "                        #counting  positive degree adverbs      \n",
    "                        parsed_tweet['padv'] = TwitterClient.get_padv(tweet.text)\n",
    "                        #counting negative degree adverbs\n",
    "                        parsed_tweet['nadv'] = TwitterClient.get_nadv(tweet.text)\n",
    "                        parsed_tweet['result'] =0\n",
    "                        if tweet.retweet_count > 0: \n",
    "                            # if tweet has retweets, ensure that it is appended only once \n",
    "                            if parsed_tweet not in tweets:\n",
    "                                \n",
    "                                #print(parsed_tweet)\n",
    "                                tweets.append(parsed_tweet)\n",
    "                                f.write(jsonpickle.encode(parsed_tweet, unpicklable=False) )\n",
    "                                f.write('\\n')\n",
    "                                \n",
    "                        else: \n",
    "                            tweets.append(parsed_tweet) \n",
    "                            f.write(jsonpickle.encode(parsed_tweet, unpicklable=False) )\n",
    "                            f.write('\\n')\n",
    "                            \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                       \n",
    "                            \n",
    "                    tweetCount += len(new_tweets)\n",
    "                    print(\"Downloaded {0} tweets\".format(tweetCount))\n",
    "                    max_id = new_tweets[-1].id\n",
    "                except tweepy.TweepError as e:\n",
    "                    # Just exit if any error\n",
    "                    print(\"some error : \" + str(e))\n",
    "                    break\n",
    "\n",
    "            print (\"Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))\n",
    "  \n",
    "            # parsing tweets one by one \n",
    "                # empty dictionary to store required params of a tweet \n",
    "               \n",
    "   \n",
    "            # return parsed tweets \n",
    "        return tweets \n",
    " \n",
    " \n",
    "    \n",
    "  \n",
    "    \n",
    "  \n",
    "def main():    \n",
    "    # creating object of TwitterClient Class \n",
    "    api = TwitterClient() \n",
    "    # calling function to get tweets \n",
    "    tweets = api.get_tweets(query = 'I feel stressed',count=100) \n",
    "    #print(tweets[1]['text'])  \n",
    "if __name__ == \"__main__\": \n",
    "    # calling main function \n",
    "    main() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6437054631828979\n",
      "Precision: 0.694300518134715\n",
      "Recall: 0.5955555555555555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yuvaneeth\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(33.0, 0.5, 'Actual label')"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "col_names = ['pword', 'nword', 'pemoji', 'nemoji', 'punc1', 'punc2', 'punc3', 'padv', 'nadv','favorite_count','retweets_count','result']\n",
    "# load dataset\n",
    "pima = pd.read_csv('final1.csv', header=None, names=col_names)\n",
    "feature_cols = ['pword', 'nword', 'pemoji', 'nemoji', 'punc1', 'punc2', 'punc3', 'padv', 'nadv','favorite_count','retweets_count']\n",
    "X = pima[feature_cols] # Features\n",
    "y = pima.result # Target variable\n",
    "# split X and y into training and testing sets\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)\n",
    "# import the class\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "y_pred=logreg.predict(X_test)\n",
    "# import the metrics class\n",
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "# import required modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2101, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yuvaneeth\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6957210776545166\n",
      "Precision: 0.7629310344827587\n",
      "Recall: 0.5636942675159236\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "\n",
    "col_names = ['pword', 'nword', 'pemoji', 'nemoji', 'punc1', 'punc2', 'punc3', 'padv', 'nadv','favorite_count','retweets_count','result']\n",
    "# load dataset\n",
    "pima = pd.read_csv('final1.csv', header=None, names=col_names)\n",
    "feature_cols = ['pword', 'nword', 'pemoji', 'nemoji', 'punc1', 'punc2', 'punc3', 'padv', 'nadv','favorite_count','retweets_count']\n",
    "X = pima[feature_cols] # Features\n",
    "y = pima.result # Target variable\n",
    "# split X and y into training and testing sets\n",
    "X = pima[feature_cols] # Features\n",
    "y = pima.result # Target variable\n",
    "\n",
    "print(pima.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=109) # 70% training and 30% test\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='rbf') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6751188589540412\n",
      "Precision: 0.7104247104247104\n",
      "Recall: 0.5859872611464968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1c463024f28>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "col_names = ['pword', 'nword', 'pemoji', 'nemoji', 'punc1', 'punc2', 'punc3', 'padv', 'nadv','favorite_count','retweets_count','result']\n",
    "# load dataset\n",
    "pima = pd.read_csv('final1.csv', header=None, names=col_names)\n",
    "feature_cols = ['pword', 'nword', 'pemoji', 'nemoji', 'punc1', 'punc2', 'punc3', 'padv', 'nadv','favorite_count','retweets_count']\n",
    "X = pima[feature_cols] # Features\n",
    "y = pima.result # Target variable\n",
    "# split X and y into training and testing sets\n",
    "X = pima[feature_cols] # Features\n",
    "y = pima.result # Target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "# Model Recall: what percentage of positive tuples are labelled as such?\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "b = y\n",
    "c = X.favorite_count\n",
    "d = X.nemoji\n",
    "plt.scatter(d,b)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
