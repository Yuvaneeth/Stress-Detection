{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yuvaneeth/Stress-Detection/blob/master/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52nlfApOYtsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re \n",
        "import tweepy \n",
        "from tweepy import OAuthHandler \n",
        "from textblob import TextBlob \n",
        "import json\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "import pandas\n",
        "import emoji\n",
        "import jsonpickle\n",
        "\n",
        "\n",
        "\n",
        "#HappyEmoticons\n",
        "emoticons_happy = set([\n",
        "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
        "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
        "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
        "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
        "    '<3'\n",
        "    ])\n",
        "# Sad Emoticons\n",
        "emoticons_sad = set([\n",
        "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
        "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
        "    ':c', ':{', '>:\\\\', ';('\n",
        "    ])\n",
        "\n",
        "POSITIVE = [\"*O\", \"*-*\", \"*O*\", \"*o*\", \"* *\",\n",
        "                \":P\", \":D\", \":d\", \":p\",\n",
        "                \";P\", \";D\", \";d\", \";p\",\n",
        "                \":-)\", \";-)\", \":=)\", \";=)\",\n",
        "                \":<)\", \":>)\", \";>)\", \";=)\",\n",
        "                \"=}\", \":)\", \"(:;)\",\n",
        "                \"(;\", \":}\", \"{:\", \";}\",\n",
        "                \"{;:]\",\n",
        "                \"[;\", \":')\", \";')\", \":-3\",\n",
        "                \"{;\", \":]\",\n",
        "                \";-3\", \":-x\", \";-x\", \":-X\",\n",
        "                \";-X\", \":-}\", \";-=}\", \":-]\",\n",
        "                \";-]\", \":-.)\",\n",
        "                \"^_^\", \"^-^\"]\n",
        "\n",
        "NEGATIVE = [\":(\", \";(\", \":'(\",\n",
        "                \"=(\", \"={\", \"):\", \");\",\n",
        "                \")':\", \")';\", \")=\", \"}=\",\n",
        "                \";-{{\", \";-{\", \":-{{\", \":-{\",\n",
        "                \":-(\", \";-(\",\n",
        "                \":,)\", \":'{\",\n",
        "                \"[:\", \";]\"\n",
        "]\n",
        "\n",
        "emopos = [\"😀\",\"😁\",\"😂\",\"🤣\",\"😃\",\"😄\",\"😅\",\"😆\",\"😉\",\"😊\",\"😋\",\"😎\",\"😍\",\"😘\",\"🥰\",\"😗\",\"😙\",\"😚\",\"☺️\",\"🙂\",\"🤗\",\"🤩\",\"😴\"\n",
        "          \"😌\",\"😛\",\"😜\",\"😝\",\"🤤\",\"🤑\",\"😬\",\"😇\",\"🤠\",\"🤡\",\"🥳\",\"🤭\",\"🧐\",\"🤓\",\"😈\"\"👻\",\"👽\",\"🤖\",\"😺\",\"😸\",\"😹\",\"😻\",\"😽\"]\n",
        "\n",
        "\n",
        "\n",
        "emoneg = [\"🤔\",\"🤨\",\"😐\",\"😑\",\"😶\",\"🙄\",\"😏\",\"😣\",\"😥\",\"😮\",\"🤐\",\"😯\",\"😪\",\"😫\",\"😒\",\"😓\",\"😔\",\"😕\",\"🙃\",\"😲\",\"☹️\",\"🙁\",\"😖\",\"😞\",\"😟\",\"😤\",\"😢\",\"😭\",\"😦\",\"😧\",\"😨\",\"😩\",\"🤯\",\n",
        "          \"😰\",\"😱\",\"🥵\",\"🥶\",\"😳\",\"🤪\",\"😵\",\"😡\",\"😠\",\"🤬\",\"😷\",\"🤒\",\"🤕\",\"🤢\",\"🤮\",\"🤧\",\"🥴\",\"🥺\",\"🤥\",\"🤫\",\"👿\",\"👹\",\"👺\",\"💀\",\"😼\",\"🙀\",\"😿\",\"😾\"]\n",
        "\n",
        "\n",
        "  \n",
        "class TwitterClient(object): \n",
        "    ''' \n",
        "    Generic Twitter Class for sentiment analysis. \n",
        "    '''      \n",
        "        \n",
        "    \n",
        "        \n",
        "    def __init__(self): \n",
        "        ''' \n",
        "        Class constructor or initialization method. \n",
        "        '''\n",
        "        # keys and tokens from the Twitter Dev Console \n",
        "        consumer_key = 'koB9DtzCdzBpYWWWuGZaAZfQ4'\n",
        "        consumer_secret = 'sidV3z48EbrcBtnp7ga9843CVe0ddoVungGIeMMwSbvS3Fu1En'\n",
        "        access_token = '922408940616351744-XOeNML7LFT92plNNzklsN6BTLt8QjD2'\n",
        "        access_token_secret = 'RpddoO7TsSory7sFLbNYNGL9gZxlsdQ1LeCDtZ7uAoV05'\n",
        "  \n",
        "        # attempt authentication \n",
        "        try: \n",
        "            # create OAuthHandler object \n",
        "            self.auth = OAuthHandler(consumer_key, consumer_secret) \n",
        "            # set access token and secret \n",
        "            self.auth.set_access_token(access_token, access_token_secret) \n",
        "            # create tweepy API object to fetch tweets \n",
        "            self.api = tweepy.API(self.auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True) \n",
        "        except: \n",
        "            print(\"Error: Authentication Failed\") \n",
        "  \n",
        "    def clean_tweet(self, tweet): \n",
        "        ''' \n",
        "        Utility function to clean tweet text by removing links, special characters \n",
        "        using simple regex statements. \n",
        "        '''\n",
        "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split()) \n",
        "         \n",
        "  \n",
        "    def get_tweet_sentiment(self, tweet): \n",
        "        ''' \n",
        "        Utility function to classify sentiment of passed tweet \n",
        "        using textblob's sentiment method \n",
        "        '''\n",
        "        # create TextBlob object of passed tweet text \n",
        "        analysis = TextBlob(self.clean_tweet(tweet)) \n",
        "        # set sentiment \n",
        "        if analysis.sentiment.polarity > 0: \n",
        "            return 'positive'\n",
        "        elif analysis.sentiment.polarity == 0: \n",
        "            return 'neutral'\n",
        "        else: \n",
        "            return 'negative'\n",
        "                                               \n",
        "\n",
        "    def get_punc(str,pun):\n",
        "        count = 0\n",
        "        if pun =='...':\n",
        "            for j in range(0,len(str)-2):\n",
        "                if(str[j]==\".\" and str[j+1]==\".\" and str[j+2]==\".\"):\n",
        "                    count = count + 1\n",
        "        else:          \n",
        "            for i in range (0, len (str)):   \n",
        "                #Checks whether given character is a punctuation mark  \n",
        "                if str[i] in (pun):  \n",
        "                    count = count + 1\n",
        "                \n",
        "        return count\n",
        "    \n",
        "    def extract_emojis(str):\n",
        "        return ''.join(c for c in str if c in emoji.UNICODE_EMOJI)\n",
        "    \n",
        "    def get_pemoji(str):\n",
        "        count = 0\n",
        "        co = TextBlob(str)\n",
        "        for i in POSITIVE:\n",
        "            if i in co:\n",
        "                count = count +1\n",
        "        for j in emoticons_happy:\n",
        "            if i in co:\n",
        "                count = count +1\n",
        "        n = TwitterClient.extract_emojis(str)\n",
        "        for k in emopos:\n",
        "            if k in n:\n",
        "                count = count +1\n",
        "        return count\n",
        "    def get_nemoji(str):\n",
        "        count = 0\n",
        "        co = TextBlob(str)\n",
        "        for i in NEGATIVE:\n",
        "            if i in co:\n",
        "                count = count +1\n",
        "        for j in emoticons_sad:\n",
        "            if i in co:\n",
        "                count = count +1\n",
        "        n = TwitterClient.extract_emojis(str)\n",
        "        for k in emoneg:\n",
        "            if k in n:\n",
        "                count = count +1\n",
        "        return count\n",
        "    \n",
        "    def get_pwords(str):\n",
        "        count = 0\n",
        "        a = TextBlob(str)\n",
        "        for word in a.words:\n",
        "            b = TextBlob(word)            \n",
        "            if(b.sentiment.polarity>0):\n",
        "                count = count + 1\n",
        "        return count\n",
        "    def get_nwords(str):\n",
        "        count = 0\n",
        "        a = TextBlob(str)\n",
        "        for word in a.words:\n",
        "            b = TextBlob(word)            \n",
        "            if(b.sentiment.polarity>0):\n",
        "                count = count + 1\n",
        "        return count\n",
        "    def get_padv(str):\n",
        "        a = TextBlob(str)\n",
        "        if(a.sentiment.polarity>0):\n",
        "            for word in a.tags:\n",
        "                if word[1]=='JJS'or word[1]=='RBS':\n",
        "                    return 3\n",
        "                elif word[1]=='JJR'or word[1]=='RBR':\n",
        "                    return 2\n",
        "                else: \n",
        "                    return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "       \n",
        "            \n",
        "    def get_nadv(str):\n",
        "        a = TextBlob(str)\n",
        "        if(a.sentiment.polarity<0):        \n",
        "            for word in a.tags:\n",
        "                if word[1]=='JJS'or word[1]=='RBS':\n",
        "                    return -3\n",
        "                elif word[1]=='JJR'or word[1]=='RBR':\n",
        "                    return -2\n",
        "                else: \n",
        "                    return -1\n",
        "        else:\n",
        "            return 0\n",
        "    \n",
        "        \n",
        "    def get_tweets(self, query, count = 30): \n",
        "        ''' \n",
        "        Main function to fetch tweets and parse them. \n",
        "        '''\n",
        "        #empty list to store parsed tweets \n",
        "        tweets = []  \n",
        "        # call twitter api to fetch tweets \n",
        "        #fetched_tweets = self.api.search(q = query, count = count)         \n",
        "            \n",
        "        # Continue with rest of code\n",
        "        searchQuery = 'i feel stressed'  # this is what we're searching for\n",
        "        maxTweets = 2500 # Some arbitrary large number\n",
        "        tweetsPerQry = 100  # this is the max the API permits\n",
        "        fName = '5k.json' # We'll store the tweets in a text file.\n",
        "\n",
        "\n",
        "        # If results from a specific ID onwards are reqd, set since_id to that ID.\n",
        "        # else default to no lower limit, go as far back as API allows\n",
        "        sinceId = None\n",
        "            # If results only below a specific ID are, set max_id to that ID.\n",
        "            # else default to no upper limit, start from the most recent tweet matching the search query.\n",
        "        max_id = -1\n",
        "\n",
        "        tweetCount = 0\n",
        "        print(\"Downloading max {0} tweets\".format(maxTweets))\n",
        "        with open(fName, 'a') as f:\n",
        "            while tweetCount < maxTweets:\n",
        "                try:\n",
        "                    if (max_id <= 0):\n",
        "                        if (not sinceId):\n",
        "                            new_tweets = self.api.search(q=searchQuery, count=tweetsPerQry)\n",
        "                        else:\n",
        "                            new_tweets = self.api.search(q=searchQuery, count=tweetsPerQry,since_id=sinceId)\n",
        "                    else:\n",
        "                        if (not sinceId):\n",
        "                            new_tweets = self.api.search(q=searchQuery, count=tweetsPerQry,max_id=str(max_id - 1))\n",
        "                        else:\n",
        "                            new_tweets = self.api.search(q=searchQuery, count=tweetsPerQry,max_id=str(max_id - 1),since_id=sinceId)\n",
        "                    if not new_tweets:\n",
        "                        print(\"No more tweets found\")\n",
        "                        break\n",
        "                    for tweet in new_tweets:\n",
        "                        \n",
        "                        parsed_tweet = {} \n",
        "                        # saving text of tweet \n",
        "                        parsed_tweet['text'] = tweet.text \n",
        "                        # saving sentiment of tweet \n",
        "                        parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text)\n",
        "                        # savint tweet time\n",
        "                        parsed_tweet['time'] =tweet.created_at.strftime(\"%d-%b-%Y (%H:%M:%S.%f)\")\n",
        "                        #saving tweet's user id\n",
        "                        parsed_tweet['userid'] = tweet.id_str\n",
        "                        #saving emojis\n",
        "                        #parsed_tweet['emoji']=tweet.entities['symbols']\n",
        "                        #saving retweet count\n",
        "                        parsed_tweet['retweets_count'] = tweet.retweet_count\n",
        "                        #saving favorite count\n",
        "                        parsed_tweet['favorite_count'] = tweet.favorite_count\n",
        "                        #saving reply count\n",
        "                        #parsed_tweet['reply_count'] = tweet.reply_count\n",
        "                        #counting number of punctuation marks\n",
        "                        parsed_tweet['punc1']=TwitterClient.get_punc(tweet.text,'!')\n",
        "                        parsed_tweet['punc2']=TwitterClient.get_punc(tweet.text,'?')\n",
        "                        parsed_tweet['punc3']=TwitterClient.get_punc(tweet.text,'...')\n",
        "                        parsed_tweet['punc4']=TwitterClient.get_punc(tweet.text,'.') - 3*TwitterClient.get_punc(tweet.text,'...')\n",
        "                        #counting positve emojis\n",
        "                        parsed_tweet['pemoji']= TwitterClient.get_pemoji(tweet.text)\n",
        "                        #counting negative emojis\n",
        "                        parsed_tweet['nemoji'] = TwitterClient.get_nemoji(tweet.text)\n",
        "                        #counting positive words\n",
        "                        parsed_tweet['pword'] = TwitterClient.get_pwords(tweet.text)\n",
        "                        #counting negative words\n",
        "                        parsed_tweet['nword'] = TwitterClient.get_nwords(tweet.text)\n",
        "                        #counting  positive degree adverbs      \n",
        "                        parsed_tweet['padv'] = TwitterClient.get_padv(tweet.text)\n",
        "                        #counting negative degree adverbs\n",
        "                        parsed_tweet['nadv'] = TwitterClient.get_nadv(tweet.text)\n",
        "                        parsed_tweet['result'] =1\n",
        "                        if tweet.retweet_count > 0: \n",
        "                            # if tweet has retweets, ensure that it is appended only once \n",
        "                            if parsed_tweet not in tweets:\n",
        "                                \n",
        "                                #print(parsed_tweet)\n",
        "                                tweets.append(parsed_tweet)\n",
        "                                f.write(jsonpickle.encode(parsed_tweet, unpicklable=False) )\n",
        "                                f.write('\\n')\n",
        "                                \n",
        "                        else: \n",
        "                            tweets.append(parsed_tweet) \n",
        "                            f.write(jsonpickle.encode(parsed_tweet, unpicklable=False) )\n",
        "                            f.write('\\n')\n",
        "                            \n",
        "                        \n",
        "                        \n",
        "                        \n",
        "                        \n",
        "                       \n",
        "                            \n",
        "                    tweetCount += len(new_tweets)\n",
        "                    print(\"Downloaded {0} tweets\".format(tweetCount))\n",
        "                    max_id = new_tweets[-1].id\n",
        "                except tweepy.TweepError as e:\n",
        "                    # Just exit if any error\n",
        "                    print(\"some error : \" + str(e))\n",
        "                    break\n",
        "\n",
        "            print (\"Downloaded {0} tweets, Saved to {1}\".format(tweetCount, fName))\n",
        "  \n",
        "            # parsing tweets one by one \n",
        "                # empty dictionary to store required params of a tweet \n",
        "               \n",
        "   \n",
        "            # return parsed tweets \n",
        "        return tweets \n",
        " \n",
        " \n",
        "    \n",
        "  \n",
        "    \n",
        "  \n",
        "def main():    \n",
        "    # creating object of TwitterClient Class \n",
        "    api = TwitterClient() \n",
        "    # calling function to get tweets \n",
        "    tweets = api.get_tweets(query = 'I feel stressed',count=100) \n",
        "    #print(tweets[1]['text'])  \n",
        "if __name__ == \"__main__\": \n",
        "    # calling main function \n",
        "    main() \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6luBcs6RZC9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import csv\n",
        "with open('5k.json', 'r') as f:\n",
        "    #line = f.read()\n",
        "     # load it as Python dict\n",
        "    for line in f:\n",
        "        tweet = json.loads(line)\n",
        "        data=[tweet['pword'] ,tweet['nword'],tweet['pemoji'],tweet['nemoji'],tweet['punc1'],tweet['punc2'],tweet['punc3'],tweet['padv'],tweet['nadv'],tweet['favorite_count'],tweet['retweets_count'],tweet['result']]\n",
        "        with open('5k.csv', 'a',encoding='utf-8') as csvFile:\n",
        "            writer = csv.writer(csvFile)\n",
        "            writer.writerow(data)\n",
        "    \n",
        "\n",
        "\t\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6t5A6wRZFxB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "\n",
        "col_names = ['pword', 'nword', 'pemoji', 'nemoji', 'punc1', 'punc2', 'punc3', 'padv', 'nadv','favorite_count','retweets_count','result']\n",
        "\n",
        "# load dataset\n",
        "\n",
        "pima = pd.read_csv('5k.csv', header=None, names=col_names)\n",
        "\n",
        "feature_cols = ['pword', 'nword', 'pemoji', 'nemoji', 'punc1', 'punc2', 'punc3', 'padv', 'nadv','favorite_count','retweets_count']\n",
        "\n",
        "X = pima[feature_cols] # Features\n",
        "\n",
        "y = pima.result # Target variable\n",
        "\n",
        "# split X and y into training and testing sets\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)\n",
        "\n",
        "# import the class\n",
        "\n",
        "\n",
        "\n",
        "# instantiate the model (using the default parameters)\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "\n",
        "\n",
        "# fit the model with data\n",
        "\n",
        "logreg.fit(X_train,y_train)\n",
        "\n",
        "\n",
        "\n",
        "y_pred=logreg.predict(X_test)\n",
        "\n",
        "# import the metrics class\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "\n",
        "cnf_matrix\n",
        "\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
        "\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
        "\n",
        "# import required modules\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "class_names=[0,1] # name  of classes\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "tick_marks = np.arange(len(class_names))\n",
        "\n",
        "plt.xticks(tick_marks, class_names)\n",
        "\n",
        "plt.yticks(tick_marks, class_names)\n",
        "\n",
        "# create heatmap\n",
        "\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
        "\n",
        "ax.xaxis.set_label_position(\"top\")\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.title('Confusion matrix', y=1.1)\n",
        "\n",
        "plt.ylabel('Actual label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIP-IInEZJ40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn import metrics\n",
        "\n",
        "col_names = ['pword', 'nword', 'pemoji', 'nemoji', 'punc1', 'punc2', 'punc3', 'padv', 'nadv','favorite_count','retweets_count','result']\n",
        "# load dataset\n",
        "pima = pd.read_csv('5k.csv', header=None, names=col_names)\n",
        "feature_cols = ['pword', 'nword', 'pemoji', 'nemoji', 'punc1', 'punc2', 'punc3', 'padv', 'nadv','favorite_count','retweets_count']\n",
        "X = pima[feature_cols] # Features\n",
        "y = pima.result # Target variable\n",
        "# split X and y into training and testing sets\n",
        "X = pima[feature_cols] # Features\n",
        "y = pima.result # Target variable\n",
        "\n",
        "print(pima.shape)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=109) # 70% training and 30% test\n",
        "#Create a svm Classifier\n",
        "clf = svm.SVC(kernel='rbf') # Linear Kernel\n",
        "\n",
        "#Train the model using the training sets\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-iLKBG6ZORv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "col_names = ['pword', 'nword', 'pemoji', 'nemoji', 'punc1', 'punc2', 'punc3', 'padv', 'nadv','favorite_count','retweets_count','result']\n",
        "# load dataset\n",
        "pima = pd.read_csv('5k.csv', header=None, names=col_names)\n",
        "feature_cols = ['pword', 'nword', 'pemoji', 'nemoji', 'punc1', 'punc2', 'punc3', 'padv', 'nadv','favorite_count','retweets_count']\n",
        "X = pima[feature_cols] # Features\n",
        "y = pima.result # Target variable\n",
        "# split X and y into training and testing sets\n",
        "X = pima[feature_cols] # Features\n",
        "y = pima.result # Target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "clf=RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
        "clf.fit(X_train,y_train)\n",
        "\n",
        "y_pred=clf.predict(X_test)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
        "\n",
        "# Model Recall: what percentage of positive tuples are labelled as such?\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}